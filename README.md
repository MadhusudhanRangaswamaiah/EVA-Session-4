# EVA-Session-4
DNN Repository includes four codes starting with Vanilla ntw and the use of techniques such as Batch Normalization, Dropout and Larger Batch Size are demonstrated in the subsequent three codes.   

## Architectural Basics

# Order
* Kernels and how do we decide the number of kernels?
* 3x3 Convolutions
* Receptive Field
* MaxPooling
* The distance of MaxPooling from Prediction
* 1x1 Convolutions
* How many layers
* When do we stop convolutions and go ahead with a larger kernel or some other alternative 
* Position of Transition Layer
* Batch Normalization
* The distance of Batch Normalization from Prediction,
* Learning Rate
* LR schedule and concept behind it
* Adam vs SGD
* Number of Epochs and when to increase them
* DropOut
* SoftMax
* Batch Size, and effects of batch size
* When to add validation checks
* How do we know our network is not going well, comparatively, very early
